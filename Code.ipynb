{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of BlackCoffer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Libraries & Packages**"
      ],
      "metadata": {
        "id": "5Cwj8HCPgcy0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv-S66TVNGqf",
        "outputId": "a8fd0eaa-590e-40c2-8ad0-7004f75982ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.7/dist-packages (3.0.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.7/dist-packages (0.2.8)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.7)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (6.0.10)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.3.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (1.1.0)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.1->newspaper3k) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.1->newspaper3k) (4.64.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.1->newspaper3k) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.1->newspaper3k) (1.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.7.1)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xlwt in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xlutils in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: xlwt>=0.7.4 in /usr/local/lib/python3.7/dist-packages (from xlutils) (1.3.0)\n",
            "Requirement already satisfied: xlrd>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from xlutils) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.7.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install xlsxwriter\n",
        "!pip install newspaper3k\n",
        "!pip install xlwt\n",
        "!pip install xlrd\n",
        "!pip install xlutils\n",
        "!pip install gdown\n",
        "!pip install -q xlrd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from openpyxl import load_workbook\n",
        "import gdown\n",
        "\n",
        "import os\n",
        "from openpyxl import load_workbook\n",
        "from google.colab import files\n",
        "#for token\n",
        "import xlwt\n",
        "import xlrd\n",
        "from xlutils.copy import copy\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords \n",
        "#for steming\n",
        "from nltk.stem import PorterStemmer\n",
        "#for Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import opinion_lexicon\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('opinion_lexicon') #Download Opinion Dictionary\n",
        "import re\n",
        "import warnings\n",
        "from typing import Iterator, List, Tuple\n",
        "from nltk.tokenize.api import TokenizerI\n",
        "from nltk.tokenize.util import align_tokens\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "\n",
        "import xlrd\n",
        "from newspaper import Article\n",
        "import pandas as pd\n",
        "import requests as rq\n",
        "from io import BytesIO\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.options.display.max_columns = None"
      ],
      "metadata": {
        "id": "MeOg4MC2NMn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Getting the Files**"
      ],
      "metadata": {
        "id": "voNwaxVzVPPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading our .txt files from drive:::::::\n",
        "\n",
        "positive_url='https://drive.google.com/file/d/11CoVQuCWjwcfwYxulDUT0oLGbDIZy3UN/view?usp=sharing'\n",
        "positive_output = \"positive-words.txt\"\n",
        "\n",
        "negative_url='https://drive.google.com/file/d/1SPxuq5hdHsKt9_mwjxefSmb04fzuekzR/view?usp=sharing'\n",
        "negative_output = \"negative-words.txt\"\n",
        "\n",
        "stop_url='https://drive.google.com/file/d/1gMOPG9gIfhUEuyRTEEoy-BdiXNSxVqTQ/view?usp=sharing'\n",
        "stop_output = \"StopWords_Generic.txt\"\n",
        "\n",
        "gdown.download(url = positive_url, output = positive_output, quiet=False, fuzzy=True)\n",
        "gdown.download(url = negative_url, output = negative_output, quiet=False, fuzzy=True)\n",
        "gdown.download(url = stop_url, output = stop_output, quiet=False, fuzzy=True)"
      ],
      "metadata": {
        "id": "wxqFe09bESxw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "90dd0b0b-b29f-4207-d826-6321a04953d1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11CoVQuCWjwcfwYxulDUT0oLGbDIZy3UN\n",
            "To: /content/positive-words.txt\n",
            "100%|██████████| 19.1k/19.1k [00:00<00:00, 8.28MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SPxuq5hdHsKt9_mwjxefSmb04fzuekzR\n",
            "To: /content/negative-words.txt\n",
            "100%|██████████| 44.8k/44.8k [00:00<00:00, 37.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gMOPG9gIfhUEuyRTEEoy-BdiXNSxVqTQ\n",
            "To: /content/StopWords_Generic.txt\n",
            "100%|██████████| 722/722 [00:00<00:00, 400kB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'StopWords_Generic.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating DF from EXCEL format**"
      ],
      "metadata": {
        "id": "tKyPQIYGgpEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# converting output file to dataframe\n",
        "df = pd.read_excel('Output Data Structure.xlsx')"
      ],
      "metadata": {
        "id": "rhcg-5yyNSqp"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main Funution for all the calculations**"
      ],
      "metadata": {
        "id": "zWjEz3hJgv6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this fun() will run for every link availabe in excel file\n",
        "path_l = []\n",
        "def fun(url, ijk):\n",
        "  global path_l\n",
        "  \n",
        "\n",
        "#for web scrapping:::::::::::\n",
        "  article = Article(url)\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  paragraph = article.text\n",
        "  sentences = nltk.sent_tokenize(paragraph)\n",
        "  sentences[:0] = [article.title+'.']\n",
        "  sentences.pop()\n",
        "\n",
        "  main_para = ' '.join(map(str, sentences))\n",
        "\n",
        "\n",
        "  ex = '.txt'\n",
        "  filename = str((ijk+1))+ex\n",
        "  f = open(filename, \"w\")\n",
        "  f.write(main_para)\n",
        "  f.close()\n",
        "\n",
        "  pwd = os.getcwd()   \n",
        "  filepath = pwd + '/'+filename\n",
        "  path_l.append(filepath)\n",
        "  \n",
        "#importing postive words and convert into LIST:::::::::::\n",
        "  my_file = open(\"positive-words.txt\", \"r\")  \n",
        "  data = my_file.read()\n",
        "  pos_list = data.split(\"\\n\")\n",
        "  my_file.close()\n",
        "\n",
        "#importing negetive words and convert into LIST:::::::::::  \n",
        "  my_file = open(\"negative-words.txt\", \"r\",  encoding = \"ISO-8859-1\")  \n",
        "  data = my_file.read()\n",
        "  neg_list = data.split(\"\\n\")\n",
        "  my_file.close()\n",
        "\n",
        "#importing stop words and convert into LIST and make every words into lower_case:::::::::::\n",
        "  my_file = open(\"StopWords_Generic.txt\", \"r\")  \n",
        "  data = my_file.read()\n",
        "  custom_stop = data.split(\"\\n\")\n",
        "  my_file.close()\n",
        "  for i in range(len(custom_stop)):\n",
        "    custom_stop[i] = custom_stop[i].lower()\n",
        "\n",
        "\n",
        "#calacukting +ve, -ve and polar score:::::::::::\n",
        "  def sentiment(sentence):\n",
        "    pos = 0\n",
        "    neg = 0\n",
        "    polar = 0\n",
        "    sub = 0\n",
        "    words = [word.lower() for word in nltk.word_tokenize(main_para)]\n",
        "  \n",
        "    for word in words:\n",
        "      if word in pos_list:\n",
        "        pos += 1\n",
        "      elif word in neg_list:\n",
        "        neg += 1\n",
        "  \n",
        "    polar = (pos - neg)/ ((pos + neg) + 0.000001)\n",
        "   \n",
        "    return [pos, neg, polar]\n",
        "\n",
        "  score = sentiment(main_para)\n",
        "\n",
        "  pos_score = score[0]\n",
        "  neg_score = score[1]\n",
        "  pol_score = score[2]\n",
        "\n",
        "#removing custom stop words and further more cleaning:::::::::::\n",
        "  ps = PorterStemmer()\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  sentences = nltk.sent_tokenize(main_para)\n",
        "  corpus = []\n",
        "  for i in range(len(sentences)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    review = [lemmatizer.lemmatize(word) for word in review if not word in set(custom_stop)]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)  \n",
        "  \n",
        "\n",
        "#words count after cleaning:::::::::::\n",
        "  words = 0\n",
        "  for i in np.arange(len(corpus)):\n",
        "    words = words + len(nltk.word_tokenize(corpus[i]))  \n",
        "  words_count_after_cleaning = words\n",
        "\n",
        "\n",
        "#subjectivity score:::::::::::::::::\n",
        "  sub = (score[0] + score[1])/ ((words) + 0.000001)\n",
        "\n",
        "#avg sentence length::::::::::::\n",
        "  sentences = nltk.sent_tokenize(main_para)\n",
        "  words = nltk.word_tokenize(main_para)\n",
        "  agv_sent_len = len(words) / len(sentences)\n",
        "\n",
        "\n",
        "#Average Word Length::::::::::::::::::::::::::\n",
        "  sentences = nltk.sent_tokenize(main_para)\n",
        "  corpus = []\n",
        "  for i in range(len(sentences)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
        "    review = review.lower()  \n",
        "    review = review.split()  \n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)  \n",
        "\n",
        "  m2 = ' '\n",
        "  m2 = m2.join(corpus)\n",
        "  lst=[x for x in m2.replace(\" \", \"\")]\n",
        "  total_char_len = len(lst)\n",
        "  total_no_words = nltk.word_tokenize(m2)\n",
        "  total_no_words = len(total_no_words)\n",
        "\n",
        "  res = total_char_len / total_no_words\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "#complex_words, syllable, complex_word%age::::::::::::::::\n",
        "  complexWord = 0\n",
        "  syllable = 0\n",
        "  complex_word_percentage = 0\n",
        "\n",
        "  for word in words:\n",
        "    vk = 0\n",
        "    if word.endswith(('es','ed')):\n",
        "      pass\n",
        "    else:\n",
        "      for w in word:\n",
        "        if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
        "          vk += 1\n",
        "          syllable += 1\n",
        "          \n",
        "      if(vk > 2):\n",
        "        complexWord += 1\n",
        "\n",
        "  if len(words) != 0:\n",
        "    complex_word_percentage = complexWord / len(words)\n",
        "  \n",
        "\n",
        "#fog index::::::::::::::::\n",
        "  fog_in = 0.4 * (agv_sent_len + complex_word_percentage)\n",
        "\n",
        "\n",
        "#Personal Pronouns::::::::::::::::::::::::::::\n",
        "  pronounRegex = re.compile(r'\\b(I|we|my|ours|(?-i:us))\\b',re.I)\n",
        "  pronouns = (pronounRegex.findall(main_para))\n",
        "  pronouns = len(pronouns)\n",
        "\n",
        "#avg words per sentence::::::::::::::::::::::::::::\n",
        "  ag_w_p_sen = agv_sent_len\n",
        "\n",
        "  return [pos_score, neg_score, pol_score, sub, agv_sent_len, round((complex_word_percentage*100),2), round(fog_in,2), ag_w_p_sen, complexWord, words_count_after_cleaning, syllable, pronouns, round(res,2)]       \n",
        "\n",
        "#used round just for ease!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BW3mhcJgN1VW"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Accesing the links and calling the fun()**"
      ],
      "metadata": {
        "id": "fGaFLUUBg4cW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To access excel file a=containg links:::::::::::::::\n",
        "loc = (\"Input.xlsx\")\n",
        "wb = xlrd.open_workbook(loc)\n",
        "sheet = wb.sheet_by_index(0)\n",
        "\n",
        "#this for loop will go through all the links and for all the links fun() will run and all the output will save in our DATAFRAME!\n",
        "for i in np.arange(sheet.nrows - 1):  \n",
        "  url = sheet.cell_value(i+1, 1)\n",
        "  result = fun(url, i)\n",
        "  df.iloc[i, 2:15] = result"
      ],
      "metadata": {
        "id": "USoVdknmNOUk"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.insert(2, 'Extract data File location', path_l)"
      ],
      "metadata": {
        "id": "zO4twvjH5JbD"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting float to int for some columns\n",
        "df['URL_ID'] = df['URL_ID'].astype(int, errors='ignore')\n",
        "df['POSITIVE SCORE'] = df['POSITIVE SCORE'].astype(int, errors='ignore')\n",
        "df['NEGATIVE SCORE'] = df['NEGATIVE SCORE'].astype(int, errors='ignore')\n",
        "df['AVG SENTENCE LENGTH'] = df['AVG SENTENCE LENGTH'].astype(int, errors='ignore')\n",
        "df['AVG NUMBER OF WORDS PER SENTENCE'] = df['AVG NUMBER OF WORDS PER SENTENCE'].astype(int, errors='ignore')\n",
        "df['COMPLEX WORD COUNT'] = df['COMPLEX WORD COUNT'].astype(int, errors='ignore')\n",
        "df['WORD COUNT'] = df['WORD COUNT'].astype(int, errors='ignore')\n",
        "df['SYLLABLE PER WORD'] = df['SYLLABLE PER WORD'].astype(int, errors='ignore')\n",
        "df['PERSONAL PRONOUNS'] = df['PERSONAL PRONOUNS'].astype(int, errors='ignore')\n",
        "df['WORD COUNT'] = df['WORD COUNT'].astype(int, errors='ignore')\n"
      ],
      "metadata": {
        "id": "9VhUWcnep0tm"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exporting**"
      ],
      "metadata": {
        "id": "21YvZYP-hFRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#exporting our final dataframe to EXCEL and download the file!\n",
        "writer = pd.ExcelWriter(\"Data Quality Associate_Intern Test OUTPUT.xlsx\", engine='xlsxwriter')\n",
        "df.to_excel(writer, index=False)\n",
        "writer.save() \n",
        "\n",
        "files.download('Data Quality Associate_Intern Test OUTPUT.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ARxR6W4TxKRx",
        "outputId": "dbfa567f-fcf7-466a-c338-5d9eef57002e"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fbae9d06-6dcc-4fa8-b298-2a3a1e952544\", \"Data Quality Associate_Intern Test OUTPUT.xlsx\", 32220)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oRfsodKFD1mo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}